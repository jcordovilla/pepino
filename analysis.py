import sqlite3
import json
import numpy as np
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from typing import List, Dict, Any, Tuple, Optional
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sentence_transformers import SentenceTransformer
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import spacy
import matplotlib.pyplot as plt
import seaborn as sns
import io
import os
from pathlib import Path
import pandas as pd
from wordcloud import WordCloud
import tempfile
from sklearn.metrics.pairwise import cosine_similarity
import asyncio
from fuzzywuzzy import fuzz
import base64
import aiosqlite

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Load spaCy model for advanced NLP
try:
    nlp = spacy.load('en_core_web_sm')
except OSError:
    print("Downloading spaCy model...")
    spacy.cli.download('en_core_web_sm')
    nlp = spacy.load('en_core_web_sm')

# Set style for better-looking graphs
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Create temp directory for graphs
TEMP_DIR = tempfile.mkdtemp()

class MessageAnalyzer:
    """Base class for message analysis with database operations"""
    
    def __init__(self, db_path: str = 'discord_messages.db'):
        """Initialize the analyzer with database connection"""
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row
        self.cursor = self.conn.cursor()
        self.embedding_model = None
        self.model_loaded = False
        self.model_load_error = None
        self._model_loading = False
        
        # Initialize database schema
        self._init_db()
        
        # Base filter to exclude sesh bot and test channels
        self.base_filter = """
            author_id != 'sesh' 
            AND author_id != '1362434210895364327'
            AND author_name != 'sesh'
            AND LOWER(author_name) != 'pepe'
            AND LOWER(author_name) != 'pepino'
            AND channel_name NOT LIKE '%test%' 
            AND channel_name NOT LIKE '%playground%' 
            AND channel_name NOT LIKE '%pg%'
        """
        
    async def ensure_model_loaded(self):
        """Ensure the embedding model is loaded"""
        if self.model_loaded:
            return True
            
        if self.model_load_error:
            return False
            
        if self._model_loading:
            return False
            
        try:
            self._model_loading = True
            # Create a new event loop for the model loading
            loop = asyncio.get_event_loop()
            # Run the model loading in a thread pool to avoid blocking
            self.embedding_model = await loop.run_in_executor(
                None,
                lambda: SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
            )
            self.model_loaded = True
            return True
        except Exception as e:
            self.model_load_error = str(e)
            return False
        finally:
            self._model_loading = False

    def get_embedding(self, text: str) -> Optional[np.ndarray]:
        """Get embedding for text if model is loaded"""
        if not self.model_loaded:
            return None
        try:
            return self.embedding_model.encode(text)
        except Exception:
            return None

    def _init_db(self):
        """Initialize database schema"""
        # Create messages table
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS messages (
                id TEXT PRIMARY KEY,
                channel_id TEXT,
                author_id TEXT,
                content TEXT,
                timestamp TEXT,
                edited_timestamp TEXT,
                is_bot BOOLEAN,
                is_system BOOLEAN,
                is_webhook BOOLEAN,
                has_attachments BOOLEAN,
                has_embeds BOOLEAN,
                has_stickers BOOLEAN,
                has_mentions BOOLEAN,
                has_reactions BOOLEAN,
                has_reference BOOLEAN,
                referenced_message_id TEXT,
                thread_id TEXT,
                thread_archived BOOLEAN,
                thread_archived_at TEXT,
                thread_auto_archive_duration INTEGER,
                thread_locked BOOLEAN,
                thread_member_count INTEGER,
                thread_message_count INTEGER,
                thread_name TEXT,
                thread_owner_id TEXT,
                thread_parent_id TEXT,
                thread_total_message_sent INTEGER,
                thread_type TEXT,
                created_at TEXT,
                updated_at TEXT
            )
        ''')
        
        # Create message_embeddings table
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS message_embeddings (
                message_id TEXT PRIMARY KEY,
                embedding BLOB,
                created_at TEXT,
                FOREIGN KEY (message_id) REFERENCES messages(id)
            )
        ''')
        
        # Create word_frequencies table
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS word_frequencies (
                word TEXT,
                frequency INTEGER,
                last_updated TEXT,
                PRIMARY KEY (word)
            )
        ''')
        
        # Create user_statistics table
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS user_statistics (
                user_id TEXT,
                channel_id TEXT,
                message_count INTEGER,
                avg_message_length REAL,
                active_hours TEXT,
                last_active TEXT,
                PRIMARY KEY (user_id, channel_id)
            )
        ''')
        
        # Create conversation_chains table
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS conversation_chains (
                root_message_id TEXT,
                last_message_id TEXT,
                message_count INTEGER,
                created_at TEXT,
                PRIMARY KEY (root_message_id)
            )
        ''')
        
        # Create message_temporal_stats table
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS message_temporal_stats (
                channel_id TEXT,
                date TEXT,
                message_count INTEGER,
                active_users INTEGER,
                avg_message_length REAL,
                PRIMARY KEY (channel_id, date)
            )
        ''')
        
        # Create indexes
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_messages_channel ON messages(channel_id)')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_messages_author ON messages(author_id)')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages(timestamp)')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_messages_reference ON messages(referenced_message_id)')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_messages_thread ON messages(thread_id)')
        
        self.conn.commit()

    def __del__(self):
        """Cleanup method to handle connection cleanup safely"""
        try:
            # For DiscordBotAnalyzer, we use async connections (self.pool)
            # The pool should be closed via the async close() method
            # We don't have a self.conn attribute, so we override the base class __del__
            pass
        except:
            # Ignore any errors during cleanup
            pass

    def preprocess_text(self, text: str) -> str:
        """Clean and preprocess text for analysis"""
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        # Remove mentions
        text = re.sub(r'<@!?\d+>', '', text)
        # Remove emojis
        text = re.sub(r'<a?:\w+:\d+>', '', text)
        # Remove special characters
        text = re.sub(r'[^\w\s]', '', text)
        # Convert to lowercase
        text = text.lower()
        return text.strip()

    def generate_message_embeddings(self, batch_size: int = 100) -> None:
        """Generate and store embeddings for messages"""
        self.cursor.execute('SELECT id, content FROM messages WHERE content IS NOT NULL')
        messages = self.cursor.fetchall()
        
        for i in range(0, len(messages), batch_size):
            batch = messages[i:i + batch_size]
            texts = [self.preprocess_text(msg['content']) for msg in batch]
            embeddings = self.embedding_model.encode(texts)
            
            for msg, embedding in zip(batch, embeddings):
                self.cursor.execute('''
                    INSERT OR REPLACE INTO message_embeddings (message_id, embedding)
                    VALUES (?, ?)
                ''', (msg['id'], embedding.tobytes()))
            
            self.conn.commit()
            print(f"Processed {i + len(batch)} messages")

    async def find_similar_messages(self, args: dict = None) -> str:
        """Find messages similar to a given message"""
        try:
            message_id = args.get('message_id') if args else None
            if not message_id:
                return "Please provide a message ID"
            
            try:
                message_id = int(message_id)  # Ensure message_id is an integer
            except ValueError:
                return "Message ID must be a number"
            
            # Get the target message
            cursor = self.conn.execute("""
                SELECT content, author_id, channel_name, timestamp
                FROM messages 
                WHERE id = ?
            """, (message_id,))
            
            target_msg = cursor.fetchone()
            if not target_msg:
                return f"Message with ID {message_id} not found"
            
            # Get other messages for comparison
            cursor = self.conn.execute("""
                SELECT id, content, author_id, channel_name, timestamp
                FROM messages 
                WHERE id != ? AND content IS NOT NULL AND content != ''
                ORDER BY timestamp DESC
                LIMIT 1000
            """, (message_id,))
            
            messages = cursor.fetchall()
            
            if not messages:
                return "No messages found for comparison"
            
            # Get embeddings
            target_embedding = await self.get_embedding(target_msg['content'])
            if target_embedding is None:
                return "Error: Could not generate embedding for target message"
            
            message_embeddings = []
            for msg in messages:
                embedding = await self.get_embedding(msg['content'])
                if embedding is not None:
                    message_embeddings.append((msg, embedding))
            
            if not message_embeddings:
                return "Error: Could not generate embeddings for comparison messages"
            
            # Calculate similarities
            similarities = []
            for msg, embedding in message_embeddings:
                similarity = cosine_similarity([target_embedding], [embedding])[0][0]
                similarities.append((msg, similarity))
            
            # Sort by similarity
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            # Format results
            result = f"**Messages Similar to Message {message_id}:**\n\n"
            result += f"Original Message: {target_msg['content'][:200]}...\n\n"
            
            result += "**Similar Messages:**\n"
            for msg, similarity in similarities[:5]:  # Top 5 similar messages
                result += f"ID: {msg['id']} (Similarity: {similarity:.2f})\n"
                result += f"Content: {msg['content'][:200]}...\n"
                result += f"Author: {msg['author_id']}\n"
                result += f"Channel: {msg['channel_name']}\n"
                result += f"Time: {msg['timestamp']}\n\n"
            
            return result
            
        except Exception as e:
            return f"Error finding similar messages: {str(e)}"

    def perform_topic_modeling(self, channel_id: Optional[str] = None) -> Dict[str, List[str]]:
        """Perform topic modeling on messages"""
        query = 'SELECT content FROM messages WHERE content IS NOT NULL'
        params = []
        if channel_id:
            query += ' AND channel_id = ?'
            params.append(channel_id)
        
        self.cursor.execute(query, params)
        messages = self.cursor.fetchall()
        
        texts = [self.preprocess_text(msg['content']) for msg in messages]
        tfidf_matrix = self.tfidf.fit_transform(texts)
        
        # Fit LDA model
        lda_output = self.lda.fit_transform(tfidf_matrix)
        
        # Get top words for each topic
        feature_names = self.tfidf.get_feature_names_out()
        topics = {}
        for topic_idx, topic in enumerate(self.lda.components_):
            top_words = [feature_names[i] for i in topic.argsort()[:-10-1:-1]]
            topics[f'Topic {topic_idx + 1}'] = top_words
        
        return topics

    async def update_word_frequencies(self, args: dict = None) -> str:
        """Update word frequency statistics"""
        try:
            # Get all messages
            cursor = self.conn.execute(f"""
                WITH filtered_messages AS (
                    SELECT content, author_id, channel_name
                    FROM messages
                    WHERE content IS NOT NULL 
                    AND content != ''
                    AND {self.base_filter}
                )
                SELECT * FROM filtered_messages
            """)
            
            messages = cursor.fetchall()
            if not messages:
                return "No messages found for word frequency analysis"
            
            # Process messages
            word_freq = {}
            for msg in messages:
                words = msg['content'].lower().split()
                for word in words:
                    if word not in stopwords.words('english') and len(word) > 2:
                        word_freq[word] = word_freq.get(word, 0) + 1
            
            # Sort by frequency
            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            
            # Format results
            result = "**Word Frequency Analysis**\n\n"
            result += "**Most Common Words:**\n"
            for word, freq in sorted_words[:20]:  # Top 20 words
                result += f"{word}: {freq} occurrences\n"
            
            return result
            
        except Exception as e:
            return f"Error updating word frequencies: {str(e)}"

    async def update_user_statistics(self, args: dict = None) -> str:
        """Update user activity statistics"""
        try:
            # Get user statistics
            cursor = self.conn.execute(f"""
                WITH filtered_messages AS (
                    SELECT *
                    FROM messages
                    WHERE {self.base_filter}
                )
                SELECT 
                    author_id,
                    COALESCE(author_display_name, author_name) as display_name,
                    COUNT(*) as message_count,
                    COUNT(DISTINCT channel_name) as channels_active,
                    AVG(LENGTH(content)) as avg_message_length
                FROM filtered_messages
                GROUP BY author_id, author_display_name, author_name
                ORDER BY message_count DESC
            """)
            
            users = cursor.fetchall()
            if not users:
                return "No user statistics available"
            
            # Format results
            result = "**User Activity Statistics**\n\n"
            for user in users:
                result += f"User {user['display_name']}:\n"
                result += f"Total Messages: {user['message_count']}\n"
                result += f"Active Channels: {user['channels_active']}\n"
                result += f"Average Message Length: {user['avg_message_length']:.1f} characters\n\n"
            
            return result
            
        except Exception as e:
            return f"Error updating user statistics: {str(e)}"

    def update_conversation_chains(self) -> Dict[str, Any]:
        """Update conversation chain analysis and return the results"""
        # Get all messages with references
        self.cursor.execute('''
            SELECT id, referenced_message_id, timestamp
            FROM messages
            WHERE referenced_message_id IS NOT NULL
            ORDER BY timestamp
        ''')
        
        messages = self.cursor.fetchall()
        chains = defaultdict(list)
        
        for msg in messages:
            chains[msg['referenced_message_id']].append(msg['id'])
        
        # Update database and collect results
        chain_stats = {
            'total_chains': len(chains),
            'longest_chain': 0,
            'avg_chain_length': 0
        }
        
        total_length = 0
        for root_id, chain in chains.items():
            last_id = chain[-1]
            chain_length = len(chain)
            chain_stats['longest_chain'] = max(chain_stats['longest_chain'], chain_length)
            total_length += chain_length
            
            self.cursor.execute('''
                INSERT OR REPLACE INTO conversation_chains 
                (root_message_id, last_message_id, message_count, created_at)
                VALUES (?, ?, ?, ?)
            ''', (root_id, last_id, chain_length, datetime.utcnow().isoformat()))
        
        self.conn.commit()
        
        if chain_stats['total_chains'] > 0:
            chain_stats['avg_chain_length'] = total_length / chain_stats['total_chains']
        
        return chain_stats

    async def update_temporal_stats(self, args: dict = None) -> str:
        """Update temporal activity statistics"""
        try:
            # Get temporal statistics
            cursor = self.conn.execute(f"""
                SELECT 
                    CAST(strftime('%H', timestamp) AS INTEGER) as hour,
                    CAST(strftime('%w', timestamp) AS INTEGER) as day_of_week,
                    DATE(timestamp) as date,
                    COUNT(*) as message_count
                FROM messages
                WHERE {self.base_filter}
                AND timestamp IS NOT NULL
                GROUP BY hour, day_of_week, date
                ORDER BY date, hour
            """)
            
            stats = cursor.fetchall()
            if not stats:
                return "No temporal statistics available"
            
            # Process statistics
            hourly_stats = {}
            daily_stats = {}
            weekly_stats = {}
            
            day_names = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
            
            for stat in stats:
                hour = stat[0]
                day_of_week = stat[1]
                message_count = stat[3]
                
                # Hourly aggregation
                if hour not in hourly_stats:
                    hourly_stats[hour] = 0
                hourly_stats[hour] += message_count
                
                # Daily aggregation
                day_name = day_names[day_of_week]
                if day_name not in daily_stats:
                    daily_stats[day_name] = 0
                daily_stats[day_name] += message_count
                
                # Weekly pattern
                if day_of_week not in weekly_stats:
                    weekly_stats[day_of_week] = 0
                weekly_stats[day_of_week] += message_count
            
            # Format results
            result = "**ðŸ“Š Temporal Activity Analysis**\n\n"
            
            # Peak hours
            if hourly_stats:
                sorted_hours = sorted(hourly_stats.items(), key=lambda x: x[1], reverse=True)
                result += "**ðŸ• Peak Activity Hours:**\n"
                for hour, count in sorted_hours[:5]:
                    time_str = f"{hour:02d}:00-{hour:02d}:59"
                    result += f"â€¢ {time_str}: {count:,} messages\n"
                result += "\n"
            
            # Daily distribution
            if daily_stats:
                result += "**ðŸ“… Activity by Day of Week:**\n"
                for day in day_names:
                    if day in daily_stats:
                        count = daily_stats[day]
                        result += f"â€¢ {day}: {count:,} messages\n"
                result += "\n"
            
            # Activity patterns
            if hourly_stats:
                result += "**â° Activity Patterns:**\n"
                
                # Morning (6-11)
                morning = sum(hourly_stats.get(h, 0) for h in range(6, 12))
                # Afternoon (12-17)
                afternoon = sum(hourly_stats.get(h, 0) for h in range(12, 18))
                # Evening (18-23)
                evening = sum(hourly_stats.get(h, 0) for h in range(18, 24))
                # Night (0-5)
                night = sum(hourly_stats.get(h, 0) for h in range(0, 6))
                
                total = morning + afternoon + evening + night
                if total > 0:
                    result += f"â€¢ Morning (06-11): {morning:,} messages ({morning/total*100:.1f}%)\n"
                    result += f"â€¢ Afternoon (12-17): {afternoon:,} messages ({afternoon/total*100:.1f}%)\n"
                    result += f"â€¢ Evening (18-23): {evening:,} messages ({evening/total*100:.1f}%)\n"
                    result += f"â€¢ Night (00-05): {night:,} messages ({night/total*100:.1f}%)\n"
            
            return result
            
        except Exception as e:
            return f"Error updating temporal statistics: {str(e)}"

    def run_all_analyses(self) -> Dict[str, Any]:
        """Run all analysis updates and return a comprehensive summary"""
        summary = {}
        
        print("Generating message embeddings...")
        self.generate_message_embeddings()
        
        print("Updating word frequencies...")
        self.update_word_frequencies()
        
        print("Updating user statistics...")
        self.update_user_statistics()
        
        print("Updating conversation chains...")
        self.update_conversation_chains()
        
        print("Updating temporal statistics...")
        self.update_temporal_stats()
        
        print("Performing topic modeling...")
        summary['topics'] = self.perform_topic_modeling()
        
        # Get overall statistics
        self.cursor.execute('''
            SELECT 
                COUNT(*) as total_messages,
                COUNT(DISTINCT author_id) as total_users,
                COUNT(DISTINCT channel_id) as total_channels,
                AVG(LENGTH(content)) as avg_message_length
            FROM messages
        ''')
        summary['overall_stats'] = dict(self.cursor.fetchone())
        
        # Get top channels
        self.cursor.execute('''
            SELECT 
                channel_id,
                channel_name,
                COUNT(*) as message_count,
                COUNT(DISTINCT author_id) as active_users
            FROM messages
            GROUP BY channel_id
            ORDER BY message_count DESC
            LIMIT 5
        ''')
        summary['top_channels'] = [dict(row) for row in self.cursor.fetchall()]
        
        # Get top users
        self.cursor.execute('''
            SELECT 
                author_id,
                author_name,
                COUNT(*) as message_count,
                AVG(LENGTH(content)) as avg_length
            FROM messages
            GROUP BY author_id
            ORDER BY message_count DESC
            LIMIT 5
        ''')
        summary['top_users'] = [dict(row) for row in self.cursor.fetchall()]
        
        # Get temporal patterns
        self.cursor.execute('''
            SELECT 
                date(timestamp) as date,
                COUNT(*) as message_count,
                COUNT(DISTINCT author_id) as active_users
            FROM messages
            GROUP BY date(timestamp)
            ORDER BY date DESC
            LIMIT 7
        ''')
        summary['recent_activity'] = [dict(row) for row in self.cursor.fetchall()]
        
        return summary

    def create_activity_graph(self, data: pd.DataFrame) -> str:
        """Create activity over time graph"""
        plt.figure(figsize=(12, 6))
        
        # Plot message count
        plt.plot(data['date'], data['message_count'], label='Messages', linewidth=2)
        
        # Plot active users
        plt.plot(data['date'], data['active_users'], label='Active Users', linewidth=2)
        
        plt.title('Message Activity Over Time', fontsize=14, pad=20)
        plt.xlabel('Date', fontsize=12)
        plt.ylabel('Count', fontsize=12)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        
        # Rotate x-axis labels for better readability
        plt.xticks(rotation=45)
        
        # Adjust layout to prevent label cutoff
        plt.tight_layout()
        
        # Save the graph
        graph_path = os.path.join(TEMP_DIR, 'activity_graph.png')
        plt.savefig(graph_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return graph_path

    def create_channel_activity_pie(self, data: pd.DataFrame) -> str:
        """Create channel activity pie chart"""
        plt.figure(figsize=(10, 10))
        
        # Create pie chart
        plt.pie(data['message_count'], 
                labels=data['channel_name'],
                autopct='%1.1f%%',
                startangle=90,
                shadow=True)
        
        plt.title('Channel Activity Distribution', fontsize=14, pad=20)
        
        # Save the graph
        graph_path = os.path.join(TEMP_DIR, 'channel_activity.png')
        plt.savefig(graph_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return graph_path

    def create_user_activity_bar(self, data: pd.DataFrame) -> str:
        """Create user activity bar chart"""
        plt.figure(figsize=(12, 6))
        
        # Create bar chart
        bars = plt.bar(data['username'], data['message_count'])
        
        # Add value labels on top of bars
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height):,}',
                    ha='center', va='bottom')
        
        plt.title('Top Users by Message Count', fontsize=14, pad=20)
        plt.xlabel('User', fontsize=12)
        plt.ylabel('Message Count', fontsize=12)
        
        # Rotate x-axis labels for better readability
        plt.xticks(rotation=45, ha='right')
        
        # Adjust layout
        plt.tight_layout()
        
        # Save the graph
        graph_path = os.path.join(TEMP_DIR, 'user_activity.png')
        plt.savefig(graph_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return graph_path

    def create_word_cloud(self, word_freq: Dict[str, int]) -> str:
        """Create word cloud from word frequencies"""
        plt.figure(figsize=(12, 8))
        
        # Generate word cloud
        wordcloud = WordCloud(width=1200, height=800,
                            background_color='white',
                            max_words=100,
                            contour_width=3,
                            contour_color='steelblue')
        
        wordcloud.generate_from_frequencies(word_freq)
        
        # Display the word cloud
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title('Most Common Words', fontsize=14, pad=20)
        
        # Save the graph
        graph_path = os.path.join(TEMP_DIR, 'word_cloud.png')
        plt.savefig(graph_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return graph_path

    def cleanup_temp_files(self):
        """Clean up temporary graph files"""
        for file in os.listdir(TEMP_DIR):
            try:
                os.remove(os.path.join(TEMP_DIR, file))
            except:
                pass
        try:
            os.rmdir(TEMP_DIR)
        except:
            pass

    async def get_available_channels(self) -> List[str]:
        """Get list of available channels in the database (ordered by activity)"""
        try:
            await self.initialize()
            async with self.pool.execute("""
                SELECT 
                    channel_name,
                    COUNT(*) as message_count
                FROM messages 
                WHERE channel_name IS NOT NULL 
                AND channel_name != ''
                AND channel_name NOT LIKE '%test%'
                AND channel_name NOT LIKE '%playground%'
                AND channel_name NOT LIKE '%pg%'
                GROUP BY channel_name
                ORDER BY message_count DESC, channel_name
            """) as cursor:
                channels = await cursor.fetchall()
                return [channel[0] for channel in channels if channel[0]]
        except Exception as e:
            print(f"Error getting available channels: {str(e)}")
            return []

    async def get_available_users(self) -> List[str]:
        """Get list of available users in the database (display names preferred)"""
        try:
            await self.initialize()
            async with self.pool.execute("""
                SELECT DISTINCT 
                    COALESCE(author_display_name, author_name) as display_name,
                    COUNT(*) as message_count
                FROM messages 
                WHERE author_name IS NOT NULL 
                AND author_id != 'sesh'
                AND author_id != '1362434210895364327'
                AND author_name != 'sesh'
                AND LOWER(author_name) != 'pepe'
                AND LOWER(author_name) != 'pepino'
                AND COALESCE(author_display_name, author_name) IS NOT NULL
                GROUP BY COALESCE(author_display_name, author_name)
                ORDER BY message_count DESC, display_name
            """) as cursor:
                users = await cursor.fetchall()
                return [user[0] for user in users if user[0]]
        except Exception as e:
            print(f"Error getting available users: {str(e)}")
            return []


class DiscordBotAnalyzer(MessageAnalyzer):
    """Advanced Discord message analyzer with statistical and semantic analysis"""
    
    def __init__(self):
        """Initialize the analyzer with database connection"""
        self.db_path = 'discord_messages.db'  # Default database path
        self.pool = None
        self.base_filter = """
            channel_name NOT LIKE '%test%'
            AND channel_name NOT LIKE '%playground%'
            AND author_id != 'sesh'
            AND author_id != '1362434210895364327'
            AND author_name != 'sesh'
            AND LOWER(author_name) != 'pepe'
            AND LOWER(author_name) != 'pepino'
        """
        # Initialize NLTK resources
        try:
            import nltk
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            import nltk
            nltk.download('punkt')
        try:
            import nltk
            nltk.data.find('corpora/stopwords')
        except LookupError:
            import nltk
            nltk.download('stopwords')
        
        # Create temp directory if it doesn't exist
        import os
        os.makedirs('temp', exist_ok=True)

    async def initialize(self):
        """Initialize the database connection pool"""
        if self.pool is None:
            try:
                import aiosqlite
                self.pool = await aiosqlite.connect(self.db_path)
                print(f"Database connection established to {self.db_path}")
            except Exception as e:
                print(f"Error connecting to database: {str(e)}")
                raise

    async def close(self):
        """Close the database connection"""
        if self.pool:
            await self.pool.close()
            self.pool = None
            print("Database connection closed")

    async def __aenter__(self):
        """Async context manager entry"""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.close()

    def format_timestamp(self, timestamp: str) -> str:
        """Format a timestamp string to a readable format"""
        try:
            from datetime import datetime
            # Handle different timestamp formats
            if 'T' in timestamp:
                dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            else:
                dt = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')
            return dt.strftime('%Y-%m-%d %H:%M')
        except:
            return timestamp

    async def get_channel_name_mapping(self, bot_guilds=None) -> Dict[str, str]:
        """Create a mapping of old database channel names to current Discord channel names"""
        try:
            await self.initialize()
            
            # Get all channel names from database
            async with self.pool.execute("""
                SELECT DISTINCT channel_name
                FROM messages 
                WHERE channel_name IS NOT NULL 
                AND channel_name != ''
                ORDER BY channel_name
            """) as cursor:
                db_channels = await cursor.fetchall()
                db_channel_names = [ch[0] for ch in db_channels if ch[0]]
            
            # If bot guilds are provided, get current Discord channel names
            current_channels = {}
            if bot_guilds:
                for guild in bot_guilds:
                    for channel in guild.channels:
                        if hasattr(channel, 'name'):
                            current_channels[channel.name] = channel.name
                            # Also map with common prefixes that might be used
                            current_channels[f"#{channel.name}"] = channel.name
                            current_channels[f"ðŸ›{channel.name}"] = channel.name
                            current_channels[f"ðŸ¦¾{channel.name}"] = channel.name
                            current_channels[f"ðŸ˜{channel.name}"] = channel.name
            
            # Create mapping: old_name -> new_name
            channel_mapping = {}
            for db_name in db_channel_names:
                # First, try exact match
                if db_name in current_channels:
                    channel_mapping[db_name] = current_channels[db_name]
                    continue
                
                # Try without emoji prefixes
                clean_db_name = db_name
                for prefix in ['ðŸ›', 'ðŸ¦¾', 'ðŸ˜', '#']:
                    if clean_db_name.startswith(prefix):
                        clean_db_name = clean_db_name[len(prefix):]
                        break
                
                # Look for matches in current channels
                best_match = None
                for current_name in current_channels.values():
                    if clean_db_name == current_name:
                        best_match = current_name
                        break
                    elif clean_db_name.lower() == current_name.lower():
                        best_match = current_name
                        break
                    elif clean_db_name in current_name or current_name in clean_db_name:
                        best_match = current_name
                
                if best_match:
                    channel_mapping[db_name] = best_match
                else:
                    # Keep original name if no match found
                    channel_mapping[db_name] = db_name
            
            return channel_mapping
            
        except Exception as e:
            print(f"Error creating channel name mapping: {str(e)}")
            return {}

    async def get_available_channels_with_mapping(self, bot_guilds=None) -> List[str]:
        """Get available channels with current Discord names when possible"""
        try:
            await self.initialize()
            
            # Get channel mapping
            channel_mapping = await self.get_channel_name_mapping(bot_guilds)
            
            # Get channels ordered by activity from database
            async with self.pool.execute("""
                SELECT 
                    channel_name,
                    COUNT(*) as message_count
                FROM messages 
                WHERE channel_name IS NOT NULL 
                AND channel_name != ''
                AND channel_name NOT LIKE '%test%'
                AND channel_name NOT LIKE '%playground%'
                AND channel_name NOT LIKE '%pg%'
                GROUP BY channel_name
                ORDER BY message_count DESC, channel_name
            """) as cursor:
                channels = await cursor.fetchall()
            
            # Map to current names and deduplicate
            mapped_channels = []
            seen = set()
            
            for channel, count in channels:
                if channel:
                    # Use mapped name if available, otherwise original
                    current_name = channel_mapping.get(channel, channel)
                    if current_name not in seen:
                        mapped_channels.append(current_name)
                        seen.add(current_name)
            
            return mapped_channels
            
        except Exception as e:
            print(f"Error getting available channels with mapping: {str(e)}")
            # Fallback to original method
            return await self.get_available_channels()

    async def analyze_topics_spacy(self, args: dict = None) -> str:
        """Analyze topics in messages using advanced spaCy NLP with trend analysis"""
        try:
            await self.initialize()
            
            # Import required libraries
            try:
                import spacy
                from collections import Counter, defaultdict
                import re
                from datetime import datetime, timedelta
                
                # Load spaCy model
                try:
                    nlp = spacy.load("en_core_web_sm")
                except OSError:
                    return "spaCy English model not found. Please install it with: python -m spacy download en_core_web_sm"
                    
            except ImportError:
                return "spaCy not installed. Please install it with: pip install spacy"
            
            # Get channel filter if provided
            channel_filter = None
            if args and "channel_name" in args:
                channel_filter = args["channel_name"]
                
                # Get all available channels
                async with self.pool.execute(f"""
                    SELECT DISTINCT channel_name 
                    FROM messages 
                    WHERE {self.base_filter}
                """) as cursor:
                    channels = await cursor.fetchall()
                    channel_names = [ch[0] for ch in channels]
                
                if channel_filter not in channel_names:
                    matches = [ch for ch in channel_names if channel_filter.lower() in ch.lower()]
                    if matches:
                        channel_filter = matches[0]
                    else:
                        return f"Channel '{channel_filter}' not found. Available channels: {', '.join(channel_names[:10])}"
            
            # Get messages with timestamps for trend analysis
            if channel_filter:
                async with self.pool.execute(f"""
                    SELECT content, timestamp, author_name
                    FROM messages 
                    WHERE channel_name = ? AND {self.base_filter}
                    AND content IS NOT NULL AND content != ''
                    AND LENGTH(content) > 30
                    ORDER BY timestamp DESC 
                    LIMIT 800
                """, (channel_filter,)) as cursor:
                    messages = await cursor.fetchall()
            else:
                async with self.pool.execute(f"""
                    SELECT content, timestamp, author_name
                    FROM messages 
                    WHERE {self.base_filter}
                    AND content IS NOT NULL AND content != ''
                    AND LENGTH(content) > 30
                    ORDER BY timestamp DESC 
                    LIMIT 800
                """) as cursor:
                    messages = await cursor.fetchall()
            
            if not messages:
                return "No messages found for topic analysis."
            
            # Advanced text cleaning to remove Discord noise
            def clean_content(text):
                # Remove Discord-specific patterns
                text = re.sub(r'<@[!&]?\d+>', '', text)  # Remove mentions
                text = re.sub(r'<#\d+>', '', text)  # Remove channel references
                text = re.sub(r'<:\w+:\d+>', '', text)  # Remove custom emojis
                text = re.sub(r'https?://\S+', '', text)  # Remove URLs
                text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)  # Remove code blocks
                text = re.sub(r'`[^`]+`', '', text)  # Remove inline code
                text = re.sub(r'[ðŸŽ¯ðŸ·ï¸ðŸ’¡ðŸ”—ðŸ”‘ðŸ“ðŸŒŽðŸ­]', '', text)  # Remove emoji noise
                text = re.sub(r'\b(time zone|buddy group|display name|main goal|learning topics)\b', '', text, flags=re.IGNORECASE)
                text = re.sub(r'\b(the server|the session|the recording|the future)\b', '', text, flags=re.IGNORECASE)
                text = re.sub(r'\b(messages?|channel|group|topic|session|meeting)\b', '', text, flags=re.IGNORECASE)
                text = re.sub(r'\b\d+\s*(minutes?|hours?|days?|weeks?|months?)\b', '', text, flags=re.IGNORECASE)
                text = re.sub(r'\s+', ' ', text).strip()
                return text
            
            # Process messages with enhanced analysis
            docs = []
            technical_terms = Counter()
            business_concepts = Counter()
            innovation_indicators = Counter()
            discussion_themes = defaultdict(list)
            temporal_trends = defaultdict(list)
            
            # Define domain-specific patterns
            tech_patterns = [
                r'\b(AI|ML|LLM|GPT|algorithm|model|neural|API|cloud|automation|pipeline|framework)\b',
                r'\b(python|javascript|typescript|react|node|docker|kubernetes|aws|azure)\b',
                r'\b(database|sql|nosql|analytics|visualization|dashboard|metrics)\b'
            ]
            
            business_patterns = [
                r'\b(strategy|roadmap|KPI|ROI|revenue|growth|market|customer|client)\b',
                r'\b(product|service|solution|platform|integration|deployment|scale)\b',
                r'\b(team|collaboration|workflow|process|efficiency|optimization)\b'
            ]
            
            innovation_pattern_list = [
                r'\b(innovation|transformation|disruption|breakthrough|cutting.edge)\b',
                r'\b(future|trend|emerging|next.gen|state.of.the.art|revolutionary)\b',
                r'\b(experiment|prototype|pilot|proof.of_concept|MVP|beta)\b'
            ]
            
            # Process messages with advanced spaCy analysis
            docs = []
            technical_terms = Counter()
            business_concepts = Counter()
            innovation_indicators = Counter()
            complex_phrases = Counter()
            semantic_concepts = defaultdict(list)
            discussion_themes = defaultdict(list)
            temporal_trends = defaultdict(list)
            sentence_contexts = []
            
            # Advanced spaCy patterns for complex phrase detection
            def extract_complex_concepts(doc):
                """Extract complex multi-word concepts using dependency parsing"""
                concepts = []
                
                # Extract compound subjects with their predicates
                for token in doc:
                    if token.dep_ == "nsubj" and token.pos_ in ["NOUN", "PROPN"]:
                        # Get the full noun phrase subject
                        subject_span = doc[token.left_edge.i:token.right_edge.i+1]
                        
                        # Get the predicate (verb and its objects)
                        if token.head.pos_ == "VERB":
                            verb = token.head
                            predicate_parts = [verb.text]
                            
                            # Add direct objects, prepositional objects
                            for child in verb.children:
                                if child.dep_ in ["dobj", "pobj", "attr", "prep"]:
                                    obj_span = doc[child.left_edge.i:child.right_edge.i+1]
                                    predicate_parts.append(obj_span.text)
                            
                            if len(predicate_parts) > 1:
                                full_concept = f"{subject_span.text} {' '.join(predicate_parts)}"
                                if len(full_concept.split()) >= 3 and len(full_concept) > 15:
                                    concepts.append(full_concept.lower().strip())
                
                # Extract complex noun phrases with modifiers
                for chunk in doc.noun_chunks:
                    # Get extended noun phrases including prepositional phrases
                    extended_phrase = chunk.text
                    
                    # Look for prepositional phrases attached to this chunk
                    for token in chunk:
                        for child in token.children:
                            if child.dep_ == "prep":
                                prep_phrase = doc[child.i:child.right_edge.i+1]
                                extended_phrase += f" {prep_phrase.text}"
                    
                    if len(extended_phrase.split()) >= 3 and len(extended_phrase) > 20:
                        concepts.append(extended_phrase.lower().strip())
                
                # Extract technical compounds (noun + noun + noun patterns)
                for i, token in enumerate(doc[:-2]):
                    if (token.pos_ in ["NOUN", "PROPN"] and 
                        doc[i+1].pos_ in ["NOUN", "PROPN", "ADJ"] and 
                        doc[i+2].pos_ in ["NOUN", "PROPN"]):
                        
                        # Check if they form a meaningful technical term
                        compound = f"{token.text} {doc[i+1].text} {doc[i+2].text}"
                        
                        # Look ahead for even longer compounds
                        j = i + 3
                        while j < len(doc) and doc[j].pos_ in ["NOUN", "PROPN"] and j < i + 6:
                            compound += f" {doc[j].text}"
                            j += 1
                        
                        if len(compound.split()) >= 3:
                            concepts.append(compound.lower())
                
                return concepts
            
            def extract_semantic_relationships(doc):
                """Extract semantic relationships using dependency parsing"""
                relationships = []
                
                # Find cause-effect relationships
                for token in doc:
                    if token.lemma_ in ["cause", "lead", "result", "enable", "drive", "impact"]:
                        # Get what causes what
                        cause = None
                        effect = None
                        
                        for child in token.children:
                            if child.dep_ in ["nsubj", "nsubjpass"]:
                                cause_span = doc[child.left_edge.i:child.right_edge.i+1]
                                cause = cause_span.text
                            elif child.dep_ in ["dobj", "attr"]:
                                effect_span = doc[child.left_edge.i:child.right_edge.i+1]
                                effect = effect_span.text
                        
                        if cause and effect and len(f"{cause} â†’ {effect}") > 10:
                            relationships.append(f"{cause} â†’ {effect}")
                
                # Find comparative relationships
                for token in doc:
                    if token.pos_ == "ADJ" and token.dep_ == "acomp":
                        # Get what's being compared
                        subject = None
                        for sibling in token.head.children:
                            if sibling.dep_ == "nsubj":
                                subj_span = doc[sibling.left_edge.i:sibling.right_edge.i+1]
                                subject = subj_span.text
                        
                        if subject:
                            comparison = f"{subject} is {token.text}"
                            if len(comparison) > 8:
                                relationships.append(comparison)
                
                return relationships
            
            # Define advanced domain patterns
            tech_patterns = [
                r'\b(AI|ML|LLM|GPT|algorithm|model|neural|API|cloud|automation|pipeline|framework)\b',
                r'\b(python|javascript|typescript|react|node|docker|kubernetes|aws|azure)\b',
                r'\b(database|sql|nosql|analytics|visualization|dashboard|metrics)\b'
            ]
            
            business_patterns = [
                r'\b(strategy|roadmap|KPI|ROI|revenue|growth|market|customer|client)\b',
                r'\b(product|service|solution|platform|integration|deployment|scale)\b',
                r'\b(team|collaboration|workflow|process|efficiency|optimization)\b'
            ]
            
            innovation_pattern_list = [
                r'\b(innovation|transformation|disruption|breakthrough|cutting.edge)\b',
                r'\b(future|trend|emerging|next.gen|state.of.the.art|revolutionary)\b',
                r'\b(experiment|prototype|pilot|proof.of_concept|MVP|beta)\b'
            ]
            
            for i, msg in enumerate(messages):
                content, timestamp, author = msg[0], msg[1], msg[2]
                
                # Skip very short or formulaic messages
                if len(content.split()) < 6:
                    continue
                    
                cleaned_content = clean_content(content)
                if len(cleaned_content.split()) < 4:
                    continue
                
                try:
                    doc = nlp(cleaned_content)
                    
                    # Store sentence-level context for later analysis
                    for sent in doc.sents:
                        if len(sent.text.split()) >= 5:
                            sentence_contexts.append({
                                'text': sent.text,
                                'author': author,
                                'timestamp': timestamp,
                                'main_concepts': [token.lemma_ for token in sent if token.pos_ in ['NOUN', 'PROPN'] and not token.is_stop]
                            })
                    
                    docs.append((doc, timestamp, author, content))
                    
                    # Extract complex concepts using advanced spaCy features
                    complex_concepts = extract_complex_concepts(doc)
                    for concept in complex_concepts:
                        if len(concept.split()) >= 3:
                            complex_phrases[concept] += 1
                            
                            # Categorize by semantic content
                            if any(tech in concept for tech in ['ai', 'ml', 'data', 'algorithm', 'api', 'tech', 'system', 'platform', 'tool']):
                                semantic_concepts['Advanced Technology'].append(concept)
                            elif any(biz in concept for biz in ['business', 'strategy', 'market', 'customer', 'revenue', 'growth', 'team', 'process']):
                                semantic_concepts['Business Strategy'].append(concept)
                            elif any(inn in concept for inn in ['innovation', 'future', 'transform', 'disrupt', 'emerging', 'new', 'next']):
                                semantic_concepts['Innovation & Future'].append(concept)
                            else:
                                semantic_concepts['General Discussion'].append(concept)
                    
                    # Extract semantic relationships
                    relationships = extract_semantic_relationships(doc)
                    for rel in relationships:
                        semantic_concepts['Cause & Effect'].append(rel)
                    
                    # Extract simple patterns for baseline
                    for pattern in tech_patterns:
                        matches = re.findall(pattern, cleaned_content, re.IGNORECASE)
                        for match in matches:
                            technical_terms[match.lower()] += 1
                    
                    for pattern in business_patterns:
                        matches = re.findall(pattern, cleaned_content, re.IGNORECASE)
                        for match in matches:
                            business_concepts[match.lower()] += 1
                    
                    for pattern in innovation_pattern_list:
                        matches = re.findall(pattern, cleaned_content, re.IGNORECASE)
                        for match in matches:
                            innovation_indicators[match.lower()] += 1
                    
                    # Time-based trend analysis with concepts
                    try:
                        msg_date = datetime.fromisoformat(timestamp.replace('Z', '+00:00')).date()
                        week_key = msg_date.strftime('%Y-W%U')
                        
                        # Add complex concepts to temporal trends
                        temporal_trends[week_key].extend([concept.split()[0] for concept in complex_concepts if concept])
                        
                    except:
                        pass
                
                except Exception as e:
                    continue
            
            if not docs:
                return "No meaningful content found for analysis."
            
            # Advanced topic clustering using semantic similarity
            semantic_clusters = defaultdict(list)
            conversation_flows = []
            
            # Analyze conversation flows (who responds to whom about what)
            for i, (doc1, ts1, author1, content1) in enumerate(docs[:100]):
                doc1_concepts = [token.lemma_.lower() for token in doc1 
                               if token.pos_ in ['NOUN', 'ADJ', 'VERB'] and not token.is_stop]
                
                for j, (doc2, ts2, author2, content2) in enumerate(docs[:100]):
                    if i >= j or author1 == author2:
                        continue
                    
                    doc2_concepts = [token.lemma_.lower() for token in doc2 
                                   if token.pos_ in ['NOUN', 'ADJ', 'VERB'] and not token.is_stop]
                    
                    # Calculate semantic overlap
                    set1, set2 = set(doc1_concepts), set(doc2_concepts)
                    if len(set1.union(set2)) > 0:
                        similarity = len(set1.intersection(set2)) / len(set1.union(set2))
                        if similarity > 0.4:  # Higher threshold for quality
                            common_concepts = list(set1.intersection(set2))[:3]
                            if common_concepts:
                                conversation_flows.append({
                                    'participants': [author1, author2],
                                    'concepts': common_concepts,
                                    'similarity': similarity
                                })
            
            # Trend analysis - identify emerging vs declining topics
            trend_analysis = {}
            if len(temporal_trends) >= 2:
                all_weeks = sorted(temporal_trends.keys())
                recent_weeks = all_weeks[-2:]
                older_weeks = all_weeks[:-2] if len(all_weeks) > 2 else []
                
                recent_concepts = Counter()
                older_concepts = Counter()
                
                for week in recent_weeks:
                    recent_concepts.update(temporal_trends[week])
                for week in older_weeks:
                    older_concepts.update(temporal_trends[week])
                
                # Find emerging topics (high in recent, low in older)
                emerging = []
                declining = []
                
                for concept, recent_count in recent_concepts.most_common(20):
                    older_count = older_concepts.get(concept, 0)
                    if recent_count >= 3:
                        ratio = recent_count / max(older_count, 1)
                        if ratio > 2 and older_count < recent_count:
                            emerging.append((concept, recent_count, ratio))
                        elif ratio < 0.5 and older_count > recent_count:
                            declining.append((concept, older_count, ratio))
                
                trend_analysis = {'emerging': emerging[:5], 'declining': declining[:5]}
            
            # Format sophisticated results
            result = "**ðŸ§  Advanced Topic & Trend Analysis**\n\n"
            if channel_filter:
                result += f"**Channel: #{channel_filter}**\n\n"
            
            result += f"**ðŸ“Š Analyzed {len([d for d in docs if d])} substantial messages**\n"
            
            # Add engagement metrics for topics analysis
            if channel_filter:
                # Calculate engagement for the specific channel
                engagement_query = f"""
                    SELECT 
                        COUNT(CASE WHEN referenced_message_id IS NOT NULL THEN 1 END) as total_replies,
                        COUNT(CASE WHEN referenced_message_id IS NULL THEN 1 END) as original_posts,
                        COUNT(CASE WHEN has_reactions = 1 THEN 1 END) as posts_with_reactions,
                        COUNT(*) as total_messages
                    FROM messages
                    WHERE channel_name = ? AND {self.base_filter}
                """
                
                async with self.pool.execute(engagement_query, (channel_filter,)) as cursor:
                    engagement_stats = await cursor.fetchone()
                
                if engagement_stats:
                    total_replies = engagement_stats[0] if engagement_stats[0] else 0
                    original_posts = engagement_stats[1] if engagement_stats[1] else 1
                    posts_with_reactions = engagement_stats[2] if engagement_stats[2] else 0
                    total_messages = engagement_stats[3] if engagement_stats[3] else 1
                    
                    replies_per_post = total_replies / original_posts if original_posts > 0 else 0
                    reaction_rate = (posts_with_reactions / total_messages * 100) if total_messages > 0 else 0
                    
                    result += f"**ðŸ“ˆ Engagement: {replies_per_post:.2f} replies/post, {reaction_rate:.1f}% reaction rate**\n"
            
            result += "\n"
            
            # Technical Innovation Topics
            if technical_terms:
                result += "**ðŸ”¬ Technical Innovation Topics:**\n"
                for term, count in technical_terms.most_common(10):
                    if count >= 3:
                        result += f"â€¢ {term.upper()} ({count} discussions)\n"
                result += "\n"
            
            # Business & Strategy Themes
            if business_concepts:
                result += "**ðŸ“ˆ Business & Strategy Themes:**\n"
                for concept, count in business_concepts.most_common(8):
                    if count >= 2:
                        result += f"â€¢ {concept.title()} ({count} mentions)\n"
                result += "\n"
            
            # Innovation Indicators
            if innovation_indicators:
                result += "**ï¿½ Innovation & Future Focus:**\n"
                for pattern, count in innovation_indicators.most_common(6):
                    if count >= 2:
                        result += f"â€¢ {pattern.title()} ({count} mentions)\n"
                result += "\n"
            
            # Complex Multi-word Concepts (NEW!)
            if complex_phrases:
                result += "**ðŸŽ¯ Complex Discussion Topics:**\n"
                for phrase, count in complex_phrases.most_common(12):
                    if count >= 2 and len(phrase.split()) >= 3:
                        result += f"â€¢ {phrase.title()} ({count} discussions)\n"
                result += "\n"
            
            # Semantic Concept Categories (NEW!)
            for category, concepts in semantic_concepts.items():
                if len(concepts) >= 3:
                    concept_counts = Counter(concepts)
                    top_concepts = concept_counts.most_common(6)
                    if top_concepts and any(count >= 2 for _, count in top_concepts):
                        result += f"**ðŸ” {category}:**\n"
                        for concept, count in top_concepts:
                            if count >= 2:
                                result += f"â€¢ {concept.title()} ({count}x)\n"
                        result += "\n"

            # Conversation Flow Analysis
            if conversation_flows:
                result += "**ï¿½ï¸ Key Conversation Flows:**\n"
                flow_summary = defaultdict(int)
                for flow in conversation_flows:
                    concept_key = ' + '.join(flow['concepts'][:2])
                    flow_summary[concept_key] += 1
                
                for flow, count in sorted(flow_summary.items(), key=lambda x: x[1], reverse=True)[:5]:
                    if count >= 2:
                        result += f"â€¢ {flow.title()} ({count} collaborative discussions)\n"
                result += "\n"
            
            # Trend Analysis
            if trend_analysis:
                if trend_analysis['emerging']:
                    result += "**ðŸ“ˆ Emerging Trends:**\n"
                    for concept, count, ratio in trend_analysis['emerging']:
                        result += f"â€¢ {concept.title()} ({count} mentions, {ratio:.1f}x growth)\n"
                    result += "\n"
                
                if trend_analysis['declining']:
                    result += "**ï¿½ Declining Topics:**\n"
                    for concept, count, ratio in trend_analysis['declining']:
                        result += f"â€¢ {concept.title()} (was {count} mentions)\n"
                    result += "\n"
            
            return result
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"Error analyzing topics: {str(e)}"
    
    async def extract_concepts_from_content(self, messages) -> List[str]:
        """Extract meaningful concepts from message content using NLP"""
        try:
            import spacy
            from collections import Counter
            
            # Load spaCy model
            try:
                nlp = spacy.load('en_core_web_sm')
            except OSError:
                print("spaCy model not found. Installing...")
                import subprocess
                subprocess.run([
                    "python", "-m", "spacy", "download", "en_core_web_sm"
                ], check=True)
                nlp = spacy.load('en_core_web_sm')
            
            all_concepts = []
            
            # Process messages in batches
            batch_size = 50
            for i in range(0, len(messages), batch_size):
                batch = messages[i:i + batch_size]
                
                # Combine batch content
                combined_text = " ".join([msg[0] for msg in batch if msg[0]])
                
                if not combined_text.strip():
                    continue
                
                # Process with spaCy
                doc = nlp(combined_text[:1000000])  # Limit text length
                
                # Extract different types of concepts
                concepts = []
                
                # 1. Named entities (persons, organizations, technologies)
                for ent in doc.ents:
                    if ent.label_ in ["PERSON", "ORG", "PRODUCT", "WORK_OF_ART"] and len(ent.text) > 2:
                        concepts.append(ent.text.lower().strip())
                
                # 2. Compound noun phrases (technical terms)
                for chunk in doc.noun_chunks:
                    if len(chunk.text.split()) >= 2 and len(chunk.text) > 5:
                        # Filter out common stopwords
                        if not any(word in chunk.text.lower() for word in ['this', 'that', 'some', 'any', 'the']):
                            concepts.append(chunk.text.lower().strip())
                
                # 3. Technical patterns (API, AI, ML terms)
                import re
                tech_patterns = [
                    r'\b[A-Z]{2,}\b',  # Acronyms like AI, ML, API
                    r'\b\w+(?:-\w+)+\b',  # Hyphenated terms
                    r'\b\w+(?:\.\w+)+\b',  # Dotted terms like node.js
                ]
                
                for pattern in tech_patterns:
                    matches = re.findall(pattern, combined_text)
                    concepts.extend([match.lower() for match in matches if len(match) > 2])
                
                # 4. Verb-object relationships (actions)
                for token in doc:
                    if token.pos_ == "VERB" and not token.is_stop:
                        # Find direct objects
                        for child in token.children:
                            if child.dep_ == "dobj" and child.pos_ in ["NOUN", "PROPN"]:
                                action_concept = f"{token.lemma_} {child.text}"
                                concepts.append(action_concept.lower())
                
                all_concepts.extend(concepts)
            
            # Count and return top concepts
            concept_counter = Counter(all_concepts)
            
            # Filter concepts (remove very short, very long, or common words)
            filtered_concepts = []
            for concept, count in concept_counter.most_common(50):
                if (3 <= len(concept) <= 50 and 
                    count >= 2 and 
                    concept not in ['the', 'and', 'for', 'with', 'this', 'that', 'from', 'they', 'have', 'will']):
                    filtered_concepts.append(concept)
            
            return filtered_concepts[:20]  # Return top 20 concepts
            
        except Exception as e:
            print(f"Error extracting concepts: {str(e)}")
            return []

    async def update_user_statistics(self, args: dict = None) -> str:
        """Enhanced user activity statistics with concept analysis - overrides base class method"""
        try:
            await self.initialize()
            
            # Get user statistics using async database connection
            async with self.pool.execute(f"""
                WITH filtered_messages AS (
                    SELECT *
                    FROM messages
                    WHERE {self.base_filter}
                )
                SELECT 
                    author_id,
                    COALESCE(author_display_name, author_name) as display_name,
                    COUNT(*) as message_count,
                    COUNT(DISTINCT channel_name) as channels_active,
                    AVG(LENGTH(content)) as avg_message_length,
                    MIN(timestamp) as first_message,
                    MAX(timestamp) as last_message
                FROM filtered_messages
                GROUP BY author_id, author_display_name, author_name
                ORDER BY message_count DESC
                LIMIT 15
            """) as cursor:
                users = await cursor.fetchall()
            
            if not users:
                return "No user statistics available"
            
            # Format results with enhanced information
            result = "**ðŸ“Š Enhanced User Activity Statistics**\n\n"
            
            for i, user in enumerate(users, 1):
                display_name = user[1] if user[1] else "Unknown"
                message_count = user[2]
                channels_active = user[3]
                avg_length = user[4] if user[4] else 0
                first_msg = self.format_timestamp(user[5]) if user[5] else "Unknown"
                last_msg = self.format_timestamp(user[6]) if user[6] else "Unknown"
                
                result += f"**{i}. {display_name}**\n"
                result += f"â€¢ Total Messages: {message_count:,}\n"
                result += f"â€¢ Active Channels: {channels_active}\n"
                result += f"â€¢ Average Message Length: {avg_length:.1f} characters\n"
                result += f"â€¢ Activity Period: {first_msg} â†’ {last_msg}\n"
                
                # Get user's main topics (simplified concept extraction)
                try:
                    async with self.pool.execute(f"""
                        SELECT content
                        FROM messages
                        WHERE author_id = ? AND {self.base_filter}
                        AND content IS NOT NULL 
                        AND LENGTH(content) > 20
                        ORDER BY timestamp DESC
                        LIMIT 100
                    """, (user[0],)) as cursor:
                        user_messages = await cursor.fetchall()
                    
                    if user_messages:
                        user_concepts = await self.extract_concepts_from_content(user_messages)
                        if user_concepts:
                            result += f"â€¢ Main Topics: {', '.join(user_concepts[:3])}\n"
                
                except Exception as e:
                    print(f"Error getting concepts for user {display_name}: {e}")
                
                result += "\n"
            
            # Add overall statistics
            total_messages = sum(user[2] for user in users)
            avg_channels_per_user = sum(user[3] for user in users) / len(users) if users else 0
            
            result += f"**ðŸ“ˆ Summary Statistics:**\n"
            result += f"â€¢ Top {len(users)} users contributed {total_messages:,} messages\n"
            result += f"â€¢ Average channels per active user: {avg_channels_per_user:.1f}\n"
            
            return result
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"Error updating user statistics: {str(e)}"

    async def resolve_channel_name(self, user_input: str, bot_guilds=None) -> str:
        """Resolve user input to the actual database channel name"""
        try:
            await self.initialize()
            
            # Get channel mapping (current -> old)
            channel_mapping = await self.get_channel_name_mapping(bot_guilds)
            
            # Create reverse mapping (current -> database_name)
            reverse_mapping = {}
            for db_name, current_name in channel_mapping.items():
                reverse_mapping[current_name] = db_name
            
            # Try to find the database channel name
            # 1. Direct match with database name
            async with self.pool.execute("""
                SELECT DISTINCT channel_name
                FROM messages 
                WHERE channel_name = ?
                LIMIT 1
            """, (user_input,)) as cursor:
                exact_match = await cursor.fetchone()
                if exact_match:
                    return user_input
            
            # 2. Try reverse mapping (current name -> database name)
            if user_input in reverse_mapping:
                return reverse_mapping[user_input]
            
            # 3. Try fuzzy matching with database names
            async with self.pool.execute("""
                SELECT DISTINCT channel_name
                FROM messages 
                WHERE channel_name IS NOT NULL
            """) as cursor:
                all_channels = await cursor.fetchall()
                db_channel_names = [ch[0] for ch in all_channels if ch[0]]
            
            # Case-insensitive exact match
            for db_channel in db_channel_names:
                if user_input.lower() == db_channel.lower():
                    return db_channel
            
            # Partial match
            for db_channel in db_channel_names:
                if user_input.lower() in db_channel.lower() or db_channel.lower() in user_input.lower():
                    return db_channel
            
            # If no match found, return original input
            return user_input
            
        except Exception as e:
            print(f"Error resolving channel name: {str(e)}")
            return user_input
    
    async def get_user_insights(self, user_name: str) -> str:
        """Get comprehensive insights for a specific user matching the original format"""
        try:
            await self.initialize()
            
            # Find the user by name (case-insensitive)
            async with self.pool.execute(f"""
                SELECT DISTINCT author_id, author_display_name, author_name
                FROM messages
                WHERE {self.base_filter}
                AND (LOWER(author_name) LIKE ? OR LOWER(author_display_name) LIKE ?)
                LIMIT 10
            """, (f"%{user_name.lower()}%", f"%{user_name.lower()}%")) as cursor:
                matching_users = await cursor.fetchall()
            
            if not matching_users:
                return f"âŒ No user found matching '{user_name}'"
            
            # If multiple matches, find the best one
            best_match = None
            for user in matching_users:
                author_id, display_name, author_name = user
                current_name = display_name or author_name
                if user_name.lower() == current_name.lower():
                    best_match = user
                    break
                elif user_name.lower() in current_name.lower():
                    best_match = user
            
            if not best_match:
                best_match = matching_users[0]
            
            author_id, display_name, author_name = best_match
            display_name = display_name or author_name
            
            # Get basic user statistics
            async with self.pool.execute(f"""
                SELECT 
                    COUNT(*) as total_messages,
                    COUNT(DISTINCT channel_name) as channels_active,
                    AVG(LENGTH(content)) as avg_message_length,
                    COUNT(DISTINCT DATE(timestamp)) as active_days,
                    MIN(timestamp) as first_message,
                    MAX(timestamp) as last_message
                FROM messages
                WHERE author_id = ? AND {self.base_filter}
                AND content IS NOT NULL
            """, (author_id,)) as cursor:
                stats = await cursor.fetchone()
            
            if not stats or stats[0] == 0:
                return f"âŒ No messages found for user '{display_name}'"
            
            total_messages, channels_active, avg_length, active_days, first_msg, last_msg = stats
            
            # Get detailed channel activity with average message lengths
            async with self.pool.execute(f"""
                SELECT 
                    channel_name, 
                    COUNT(*) as message_count,
                    AVG(LENGTH(content)) as avg_chars_per_message
                FROM messages
                WHERE author_id = ? AND {self.base_filter}
                AND content IS NOT NULL
                GROUP BY channel_name
                ORDER BY message_count DESC
                LIMIT 5
            """, (author_id,)) as cursor:
                channel_activity = await cursor.fetchall()
            
            # Get activity by time of day (grouped into periods)
            async with self.pool.execute(f"""
                SELECT 
                    CASE 
                        WHEN CAST(strftime('%H', timestamp) AS INTEGER) BETWEEN 0 AND 5 THEN 'Night (00-05)'
                        WHEN CAST(strftime('%H', timestamp) AS INTEGER) BETWEEN 6 AND 11 THEN 'Morning (06-11)'
                        WHEN CAST(strftime('%H', timestamp) AS INTEGER) BETWEEN 12 AND 17 THEN 'Afternoon (12-17)'
                        WHEN CAST(strftime('%H', timestamp) AS INTEGER) BETWEEN 18 AND 23 THEN 'Evening (18-23)'
                        ELSE 'Unknown'
                    END as time_period,
                    COUNT(*) as messages
                FROM messages
                WHERE author_id = ? AND {self.base_filter}
                AND timestamp IS NOT NULL
                GROUP BY time_period
                ORDER BY messages DESC
            """, (author_id,)) as cursor:
                time_activity = await cursor.fetchall()
            
            # Get recent messages for content analysis
            async with self.pool.execute(f"""
                SELECT content
                FROM messages
                WHERE author_id = ? AND {self.base_filter}
                AND content IS NOT NULL 
                AND LENGTH(content) > 20
                ORDER BY timestamp DESC
                LIMIT 200
            """, (author_id,)) as cursor:
                recent_messages = await cursor.fetchall()
            
            # Extract key concepts using advanced spaCy analysis (same as channel analysis)
            user_concepts = []
            technical_terms = []
            business_concepts = []
            innovation_indicators = []
            complex_phrases = []
            
            if recent_messages:
                try:
                    import spacy
                    from collections import Counter
                    import re
                    
                    # Load spaCy model
                    try:
                        nlp = spacy.load("en_core_web_sm")
                    except OSError:
                        print("spaCy model not found. Will use basic concept extraction.")
                        user_concepts = await self.extract_concepts_from_content(recent_messages)
                    else:
                        # Advanced text cleaning (same as channel analysis)
                        def clean_content(text):
                            text = re.sub(r'<@[!&]?\d+>', '', text)  # Remove mentions
                            text = re.sub(r'<#\d+>', '', text)  # Remove channel references
                            text = re.sub(r'<:\w+:\d+>', '', text)  # Remove custom emojis
                            text = re.sub(r'https?://\S+', '', text)  # Remove URLs
                            text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)  # Remove code blocks
                            text = re.sub(r'`[^`]+`', '', text)  # Remove inline code
                            text = re.sub(r'[ðŸŽ¯ðŸ·ï¸ðŸ’¡ðŸ”—ðŸ”‘ðŸ“ðŸŒŽðŸ­]', '', text)  # Remove emoji noise
                            text = re.sub(r'\b(time zone|buddy group|display name|main goal|learning topics)\b', '', text, flags=re.IGNORECASE)
                            text = re.sub(r'\b(the server|the session|the recording|the future)\b', '', text, flags=re.IGNORECASE)
                            text = re.sub(r'\b(messages?|channel|group|topic|session|meeting)\b', '', text, flags=re.IGNORECASE)
                            text = re.sub(r'\b\d+\s*(minutes?|hours?|days?|weeks?|months?)\b', '', text, flags=re.IGNORECASE)
                            text = re.sub(r'\s+', ' ', text).strip()
                            return text
                        
                        # Advanced concept extraction using spaCy (same patterns as channel analysis)
                        def extract_complex_concepts(doc):
                            concepts = []
                            
                            # Extract compound subjects with their predicates
                            for token in doc:
                                if token.dep_ == "nsubj" and token.pos_ in ["NOUN", "PROPN"]:
                                    subject_span = doc[token.left_edge.i:token.right_edge.i+1]
                                    if token.head.pos_ == "VERB":
                                        verb = token.head
                                        predicate_parts = [verb.text]
                                        for child in verb.children:
                                            if child.dep_ in ["dobj", "pobj", "attr", "prep"]:
                                                obj_span = doc[child.left_edge.i:child.right_edge.i+1]
                                                predicate_parts.append(obj_span.text)
                                        if len(predicate_parts) > 1:
                                            full_concept = f"{subject_span.text} {' '.join(predicate_parts)}"
                                            if len(full_concept.split()) >= 3 and len(full_concept) > 15:
                                                concepts.append(full_concept.lower().strip())
                            
                            # Extract extended noun phrases
                            for chunk in doc.noun_chunks:
                                extended_phrase = chunk.text
                                for token in chunk:
                                    for child in token.children:
                                        if child.dep_ == "prep":
                                            prep_phrase = doc[child.i:child.right_edge.i+1]
                                            extended_phrase += f" {prep_phrase.text}"
                                if len(extended_phrase.split()) >= 3 and len(extended_phrase) > 20:
                                    concepts.append(extended_phrase.lower().strip())
                            
                            # Extract technical compounds
                            for i, token in enumerate(doc[:-2]):
                                if (token.pos_ in ["NOUN", "PROPN"] and 
                                    doc[i+1].pos_ in ["NOUN", "PROPN", "ADJ"] and 
                                    doc[i+2].pos_ in ["NOUN", "PROPN"]):
                                    compound = f"{token.text} {doc[i+1].text} {doc[i+2].text}"
                                    j = i + 3
                                    while j < len(doc) and doc[j].pos_ in ["NOUN", "PROPN"] and j < i + 6:
                                        compound += f" {doc[j].text}"
                                        j += 1
                                    if len(compound.split()) >= 3:
                                        concepts.append(compound.lower())
                            
                            return concepts
                        
                        # Domain-specific patterns (same as channel analysis)
                        tech_patterns = [
                            r'\b(AI|ML|LLM|GPT|algorithm|model|neural|API|cloud|automation|pipeline|framework)\b',
                            r'\b(python|javascript|typescript|react|node|docker|kubernetes|aws|azure)\b',
                            r'\b(database|sql|nosql|analytics|visualization|dashboard|metrics)\b'
                        ]
                        
                        business_patterns = [
                            r'\b(strategy|roadmap|KPI|ROI|revenue|growth|market|customer|client)\b',
                            r'\b(product|service|solution|platform|integration|deployment|scale)\b',
                            r'\b(team|collaboration|workflow|process|efficiency|optimization)\b'
                        ]
                        
                        innovation_patterns = [
                            r'\b(innovation|transformation|disruption|breakthrough|cutting.edge)\b',
                            r'\b(future|trend|emerging|next.gen|state.of.the.art|revolutionary)\b',
                            r'\b(experiment|prototype|pilot|proof.of_concept|MVP|beta)\b'
                        ]
                        
                        # Process user's messages
                        all_complex_concepts = []
                        tech_counter = Counter()
                        business_counter = Counter()
                        innovation_counter = Counter()
                        
                        # Combine recent messages
                        combined_text = " ".join([msg[0] for msg in recent_messages if msg[0]])
                        cleaned_content = clean_content(combined_text)
                        
                        if cleaned_content and len(cleaned_content.split()) >= 10:
                            try:
                                doc = nlp(cleaned_content[:500000])  # Process user's content
                                
                                # Extract complex concepts
                                complex_concepts = extract_complex_concepts(doc)
                                all_complex_concepts.extend(complex_concepts)
                                
                                # Extract simple patterns
                                for pattern in tech_patterns:
                                    matches = re.findall(pattern, cleaned_content, re.IGNORECASE)
                                    for match in matches:
                                        tech_counter[match.lower()] += 1
                                
                                for pattern in business_patterns:
                                    matches = re.findall(pattern, cleaned_content, re.IGNORECASE)
                                    for match in matches:
                                        business_counter[match.lower()] += 1
                                
                                for pattern in innovation_patterns:
                                    matches = re.findall(pattern, cleaned_content, re.IGNORECASE)
                                    for match in matches:
                                        innovation_counter[match.lower()] += 1
                                
                            except Exception as e:
                                print(f"Error in spaCy processing for {display_name}: {e}")
                        
                        # Compile results
                        complex_phrases = [concept for concept, count in Counter(all_complex_concepts).most_common(8) if count >= 1]
                        technical_terms = [term for term, count in tech_counter.most_common(5) if count >= 1]
                        business_concepts = [concept for concept, count in business_counter.most_common(5) if count >= 1]
                        innovation_indicators = [pattern for pattern, count in innovation_counter.most_common(5) if count >= 1]
                        
                        # Combine all concepts for the traditional display
                        user_concepts = (complex_phrases[:4] + technical_terms[:2] + 
                                       business_concepts[:2] + innovation_indicators[:2])[:8]
                        
                except Exception as e:
                    print(f"Error extracting advanced concepts for {display_name}: {e}")
                    # Fallback to basic extraction
                    try:
                        user_concepts = await self.extract_concepts_from_content(recent_messages)
                    except:
                        user_concepts = []
            
            # Generate user activity chart for past 30 days
            chart_path = None
            try:
                import matplotlib.pyplot as plt
                import matplotlib.dates as mdates
                from datetime import datetime, timedelta
                import os
                
                # Get daily message counts for past 30 days for this user
                async with self.pool.execute(f"""
                    SELECT 
                        DATE(timestamp) as date,
                        COUNT(*) as messages
                    FROM messages 
                    WHERE author_id = ? AND {self.base_filter}
                    AND timestamp IS NOT NULL
                    AND DATE(timestamp) >= DATE('now', '-30 days')
                    GROUP BY DATE(timestamp)
                    ORDER BY date ASC
                """, (author_id,)) as cursor:
                    daily_activity = await cursor.fetchall()
                
                if daily_activity and len(daily_activity) > 1:
                    # Prepare data for plotting
                    dates = []
                    message_counts = []
                    
                    for date_str, count in daily_activity:
                        try:
                            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                            dates.append(date_obj)
                            message_counts.append(count)
                        except:
                            continue
                    
                    if dates and message_counts:
                        # Create the plot
                        plt.figure(figsize=(12, 6))
                        plt.bar(dates, message_counts, color='#5865F2', alpha=0.7, edgecolor='#4752C4', linewidth=1)
                        
                        # Clean user name for chart title (remove emojis and special chars)
                        clean_user_name = re.sub(r'[^\w\s-]', '', display_name)
                        
                        # Formatting
                        plt.title(f'Daily Message Activity - {clean_user_name}', fontsize=16, fontweight='bold', pad=20)
                        plt.xlabel('Date', fontsize=12)
                        plt.ylabel('Number of Messages', fontsize=12)
                        
                        # Format x-axis
                        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))
                        plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=max(1, len(dates)//10)))
                        plt.xticks(rotation=45)
                        
                        # Add grid for better readability
                        plt.grid(True, alpha=0.3, axis='y')
                        
                        # Add some statistics to the plot
                        avg_messages = sum(message_counts) / len(message_counts)
                        max_messages = max(message_counts)
                        
                        plt.axhline(y=avg_messages, color='red', linestyle='--', alpha=0.7, 
                                  label=f'Average: {avg_messages:.1f} msg/day')
                        
                        plt.legend()
                        plt.tight_layout()
                        
                        # Save the chart with sanitized filename
                        safe_user_name = re.sub(r'[^\w\s-]', '', display_name).replace(' ', '_').strip('_')
                        if not safe_user_name:
                            safe_user_name = "unknown_user"
                        
                        chart_filename = f"user_activity_{safe_user_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                        chart_path = os.path.join('temp', chart_filename)
                        
                        # Ensure temp directory exists
                        os.makedirs('temp', exist_ok=True)
                        
                        plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
                        plt.close()
                        
            except Exception as e:
                print(f"Error generating user activity chart: {e}")
            
            # Format results to match the original
            result = f"**User Analysis: {display_name}**\n\n"
            
            # General Statistics
            result += f"**ðŸ“Š General Statistics:**\n"
            result += f"â€¢ Total Messages: {total_messages}\n"
            result += f"â€¢ Active Channels: {channels_active}\n"
            result += f"â€¢ Average Message Length: {avg_length:.1f} characters\n"
            result += f"â€¢ Active Days: {active_days}\n"
            
            if first_msg and last_msg:
                # Format timestamps to match original (YYYY-MM-DD HH:MM)
                try:
                    from datetime import datetime
                    first_dt = datetime.fromisoformat(first_msg.replace('Z', '+00:00'))
                    last_dt = datetime.fromisoformat(last_msg.replace('Z', '+00:00'))
                    result += f"â€¢ First Message: {first_dt.strftime('%Y-%m-%d %H:%M')}\n"
                    result += f"â€¢ Last Message: {last_dt.strftime('%Y-%m-%d %H:%M')}\n"
                except:
                    result += f"â€¢ First Message: {self.format_timestamp(first_msg)}\n"
                    result += f"â€¢ Last Message: {self.format_timestamp(last_msg)}\n"
            
            result += "\n"
            
            # Channel Activity
            if channel_activity:
                result += f"**ðŸ“ Channel Activity:**\n"
                for channel, count, avg_chars in channel_activity:
                    result += f"â€¢ #{channel}: {count} messages (avg {avg_chars:.0f} chars)\n"
                result += "\n"
            
            # Activity by Time of Day
            if time_activity:
                result += f"**ï¿½ Activity by Time of Day:**\n"
                for period, count in time_activity:
                    result += f"â€¢ {period}: {count} messages\n"
                result += "\n"
            
            # Key Topics & Concepts
            if user_concepts:
                result += f"**ðŸ§  Key Topics & Concepts:**\n"
                # Format concepts with title case and bullet points
                formatted_concepts = []
                for concept in user_concepts[:8]:
                    # Title case each word and clean up
                    formatted_concept = ' '.join(word.capitalize() for word in concept.split())
                    formatted_concepts.append(f"â€¢ {formatted_concept}")
                
                result += '\n'.join(formatted_concepts) + "\n"
            
            # Return both text and chart path if chart was generated
            if chart_path and os.path.exists(chart_path):
                return (result, chart_path)
            else:
                return result
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"Error getting user insights: {str(e)}"
    
    async def get_channel_insights(self, channel_name: str) -> str:
        """Get comprehensive channel statistics and insights"""
        try:
            await self.initialize()
            
            # Resolve the channel name to the actual database name
            resolved_channel = await self.resolve_channel_name(channel_name)
            
            # Get basic channel statistics
            async with self.pool.execute(f"""
                SELECT 
                    COUNT(*) as total_messages,
                    COUNT(DISTINCT author_id) as unique_users,
                    AVG(LENGTH(content)) as avg_message_length,
                    MIN(timestamp) as first_message,
                    MAX(timestamp) as last_message
                FROM messages 
                WHERE channel_name = ? AND {self.base_filter}
                AND content IS NOT NULL
            """, (resolved_channel,)) as cursor:
                stats = await cursor.fetchone()
            
            if not stats or stats[0] == 0:
                # Try to find similar channel names
                async with self.pool.execute(f"""
                    SELECT DISTINCT channel_name, COUNT(*) as msg_count
                    FROM messages 
                    WHERE {self.base_filter}
                    AND LOWER(channel_name) LIKE ?
                    GROUP BY channel_name
                    ORDER BY msg_count DESC
                    LIMIT 5
                """, (f"%{channel_name.lower()}%",)) as cursor:
                    similar_channels = await cursor.fetchall()
                
                if similar_channels:
                    suggestions = ", ".join([ch[0] for ch in similar_channels])
                    return f"âŒ No messages found for channel '{channel_name}'. Did you mean: {suggestions}?"
                else:
                    return f"âŒ No channel found matching '{channel_name}'"
            
            total_messages, unique_users, avg_length, first_msg, last_msg = stats
            
            # Get engagement metrics
            async with self.pool.execute(f"""
                SELECT 
                    COUNT(CASE WHEN referenced_message_id IS NOT NULL THEN 1 END) as total_replies,
                    COUNT(CASE WHEN referenced_message_id IS NULL THEN 1 END) as original_posts,
                    COUNT(CASE WHEN has_reactions = 1 THEN 1 END) as posts_with_reactions
                FROM messages 
                WHERE channel_name = ? AND {self.base_filter}
            """, (resolved_channel,)) as cursor:
                engagement = await cursor.fetchone()
            
            total_replies, original_posts, posts_with_reactions = engagement
            replies_per_post = total_replies / original_posts if original_posts > 0 else 0
            reaction_rate = (posts_with_reactions / total_messages * 100) if total_messages > 0 else 0
            
            # Get top contributors
            async with self.pool.execute(f"""
                SELECT 
                    COALESCE(author_display_name, author_name) as display_name,
                    COUNT(*) as message_count,
                    AVG(LENGTH(content)) as avg_chars
                FROM messages 
                WHERE channel_name = ? AND {self.base_filter}
                AND content IS NOT NULL
                GROUP BY author_id, author_display_name, author_name
                ORDER BY message_count DESC
                LIMIT 5
            """, (resolved_channel,)) as cursor:
                contributors = await cursor.fetchall()
            
            # Get peak activity hours
            async with self.pool.execute(f"""
                SELECT 
                    strftime('%H', timestamp) as hour,
                    COUNT(*) as messages
                FROM messages 
                WHERE channel_name = ? AND {self.base_filter}
                AND timestamp IS NOT NULL
                GROUP BY strftime('%H', timestamp)
                ORDER BY messages DESC
                LIMIT 3
            """, (resolved_channel,)) as cursor:
                peak_hours = await cursor.fetchall()
            
            # Get activity by day of week
            async with self.pool.execute(f"""
                SELECT 
                    CASE strftime('%w', timestamp)
                        WHEN '0' THEN 'Sunday'
                        WHEN '1' THEN 'Monday'
                        WHEN '2' THEN 'Tuesday'
                        WHEN '3' THEN 'Wednesday'
                        WHEN '4' THEN 'Thursday'
                        WHEN '5' THEN 'Friday'
                        WHEN '6' THEN 'Saturday'
                    END as day_name,
                    COUNT(*) as messages
                FROM messages 
                WHERE channel_name = ? AND {self.base_filter}
                AND timestamp IS NOT NULL
                GROUP BY strftime('%w', timestamp)
                ORDER BY messages DESC
                LIMIT 3
            """, (resolved_channel,)) as cursor:
                day_activity = await cursor.fetchall()
            
            # Get recent activity (last 7 days)
            async with self.pool.execute(f"""
                SELECT 
                    DATE(timestamp) as date,
                    COUNT(*) as messages
                FROM messages 
                WHERE channel_name = ? AND {self.base_filter}
                AND timestamp IS NOT NULL
                AND DATE(timestamp) >= DATE('now', '-7 days')
                GROUP BY DATE(timestamp)
                ORDER BY date DESC
                LIMIT 7
            """, (resolved_channel,)) as cursor:
                recent_activity = await cursor.fetchall()
            
            # Get channel health metrics (activity in last week)
            async with self.pool.execute(f"""
                SELECT COUNT(DISTINCT author_id) as weekly_active
                FROM messages 
                WHERE channel_name = ? AND {self.base_filter}
                AND timestamp >= datetime('now', '-7 days')
            """, (resolved_channel,)) as cursor:
                weekly_active_result = await cursor.fetchone()
            
            weekly_active = weekly_active_result[0] if weekly_active_result else 0
            channel_amp = (weekly_active / unique_users * 100) if unique_users > 0 else 0
            
            # Get inactive users (users who posted before but not in last 7 days)
            async with self.pool.execute(f"""
                SELECT COUNT(DISTINCT author_id) as inactive_users
                FROM messages 
                WHERE channel_name = ? AND {self.base_filter}
                AND timestamp < datetime('now', '-7 days')
                AND author_id NOT IN (
                    SELECT DISTINCT author_id 
                    FROM messages 
                    WHERE channel_name = ? AND {self.base_filter}
                    AND timestamp >= datetime('now', '-7 days')
                )
            """, (resolved_channel, resolved_channel)) as cursor:
                inactive_result = await cursor.fetchone()
            
            inactive_users = inactive_result[0] if inactive_result else 0
            inactive_percentage = (inactive_users / unique_users * 100) if unique_users > 0 else 0
            
            # Get total channel members (from new channel_members table)
            total_channel_members = 0
            lurkers = 0
            participation_rate = 0
            
            try:
                async with self.pool.execute(f"""
                    SELECT COUNT(DISTINCT user_id) as total_members
                    FROM channel_members 
                    WHERE channel_name = ?
                """, (resolved_channel,)) as cursor:
                    member_result = await cursor.fetchone()
                
                if member_result and member_result[0]:
                    total_channel_members = member_result[0]
                    lurkers = total_channel_members - unique_users
                    participation_rate = (unique_users / total_channel_members * 100) if total_channel_members > 0 else 0
                    
            except Exception as e:
                # If channel_members table doesn't exist or has no data, continue without it
                print(f"Note: Channel membership data not available: {e}")
            
            # Extract top topics using advanced spaCy analysis
            top_topics = []
            try:
                import spacy
                from collections import Counter
                import re
                
                # Get content for topic analysis
                async with self.pool.execute(f"""
                    SELECT content
                    FROM messages 
                    WHERE channel_name = ? AND {self.base_filter}
                    AND content IS NOT NULL 
                    AND LENGTH(content) > 30
                    ORDER BY timestamp DESC
                    LIMIT 200
                """, (resolved_channel,)) as cursor:
                    topic_messages = await cursor.fetchall()
                
                if topic_messages:
                    try:
                        nlp = spacy.load("en_core_web_sm")
                        
                        # Advanced text cleaning (same as channel analysis)
                        def clean_content(text):
                            text = re.sub(r'<@[!&]?\d+>', '', text)  # Remove mentions
                            text = re.sub(r'<#\d+>', '', text)  # Remove channel references
                            text = re.sub(r'<:\w+:\d+>', '', text)  # Remove custom emojis
                            text = re.sub(r'https?://\S+', '', text)  # Remove URLs
                            text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)  # Remove code blocks
                            text = re.sub(r'`[^`]+`', '', text)  # Remove inline code
                            text = re.sub(r'[ðŸŽ¯ðŸ·ï¸ðŸ’¡ðŸ”—ðŸ”‘ðŸ“ðŸŒŽðŸ­]', '', text)  # Remove emoji noise
                            text = re.sub(r'\b(time zone|buddy group|display name|main goal|learning topics)\b', '', text, flags=re.IGNORECASE)
                            text = re.sub(r'\b(the server|the session|the recording|the future)\b', '', text, flags=re.IGNORECASE)
                            text = re.sub(r'\b(messages?|channel|group|topic|session|meeting)\b', '', text, flags=re.IGNORECASE)
                            text = re.sub(r'\s+', ' ', text).strip()
                            return text
                        
                        # Extract complex concepts using spaCy
                        def extract_complex_topics(doc):
                            topics = []
                            
                            # Extract compound subjects with their predicates
                            for token in doc:
                                if token.dep_ == "nsubj" and token.pos_ in ["NOUN", "PROPN"]:
                                    subject_span = doc[token.left_edge.i:token.right_edge.i+1]
                                    if token.head.pos_ == "VERB":
                                        verb = token.head
                                        predicate_parts = [verb.text]
                                        for child in verb.children:
                                            if child.dep_ in ["dobj", "pobj", "attr"]:
                                                obj_span = doc[child.left_edge.i:child.right_edge.i+1]
                                                predicate_parts.append(obj_span.text)
                                        if len(predicate_parts) > 1:
                                            full_topic = f"{subject_span.text} {' '.join(predicate_parts)}"
                                            if len(full_topic.split()) >= 2 and len(full_topic) > 10:
                                                topics.append(full_topic.lower().strip())
                            
                            # Extract extended noun phrases (technical terms, concepts)
                            for chunk in doc.noun_chunks:
                                if len(chunk.text.split()) >= 2 and len(chunk.text) > 8:
                                    # Filter out common stopwords and generic terms
                                    if not any(word in chunk.text.lower() for word in ['this', 'that', 'some', 'any', 'the', 'these', 'those']):
                                        topics.append(chunk.text.lower().strip())
                            
                            # Extract technical compounds and domain-specific terms
                            for i, token in enumerate(doc[:-1]):
                                if (token.pos_ in ["NOUN", "PROPN", "ADJ"] and 
                                    doc[i+1].pos_ in ["NOUN", "PROPN"]):
                                    compound = f"{token.text} {doc[i+1].text}"
                                    # Look ahead for longer compounds
                                    j = i + 2
                                    while j < len(doc) and doc[j].pos_ in ["NOUN", "PROPN"] and j < i + 4:
                                        compound += f" {doc[j].text}"
                                        j += 1
                                    if len(compound.split()) >= 2 and len(compound) > 6:
                                        topics.append(compound.lower())
                            
                            return topics
                        
                        # Process messages and extract topics
                        all_topics = []
                        combined_text = " ".join([msg[0] for msg in topic_messages if msg[0]])
                        cleaned_content = clean_content(combined_text)
                        
                        if cleaned_content and len(cleaned_content.split()) >= 20:
                            doc = nlp(cleaned_content[:500000])  # Limit text length
                            complex_topics = extract_complex_topics(doc)
                            all_topics.extend(complex_topics)
                        
                        # Count and filter topics
                        topic_counter = Counter(all_topics)
                        
                        # Filter for meaningful topics (avoid very short, very long, or common words)
                        filtered_topics = []
                        for topic, count in topic_counter.most_common(50):
                            if (6 <= len(topic) <= 60 and 
                                count >= 2 and 
                                len(topic.split()) >= 2 and
                                topic not in ['the', 'and', 'for', 'with', 'this', 'that', 'from', 'they', 'have', 'will', 'can', 'would', 'could'] and
                                not topic.startswith(('i ', 'you ', 'we ', 'they '))):
                                # Title case the topic for display
                                formatted_topic = ' '.join(word.capitalize() for word in topic.split())
                                filtered_topics.append(formatted_topic)
                        
                        top_topics = filtered_topics[:10]
                        
                    except OSError:
                        print("spaCy model not found for topic extraction.")
                    except Exception as e:
                        print(f"Error in topic extraction: {e}")
                        
            except Exception as e:
                print(f"Error extracting topics: {e}")
            
            # Generate activity chart for past 30 days
            chart_path = None
            try:
                import matplotlib.pyplot as plt
                import matplotlib.dates as mdates
                from datetime import datetime, timedelta
                import os
                
                # Get daily message counts for past 30 days
                async with self.pool.execute(f"""
                    SELECT 
                        DATE(timestamp) as date,
                        COUNT(*) as messages
                    FROM messages 
                    WHERE channel_name = ? AND {self.base_filter}
                    AND timestamp IS NOT NULL
                    AND DATE(timestamp) >= DATE('now', '-30 days')
                    GROUP BY DATE(timestamp)
                    ORDER BY date ASC
                """, (resolved_channel,)) as cursor:
                    daily_activity = await cursor.fetchall()
                
                if daily_activity and len(daily_activity) > 1:
                    # Prepare data for plotting
                    dates = []
                    message_counts = []
                    
                    for date_str, count in daily_activity:
                        try:
                            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                            dates.append(date_obj)
                            message_counts.append(count)
                        except:
                            continue
                    
                    if dates and message_counts:
                        # Create the plot
                        plt.figure(figsize=(12, 6))
                        plt.bar(dates, message_counts, color='#5865F2', alpha=0.7, edgecolor='#4752C4', linewidth=1)
                        
                        # Clean channel name for chart title (remove emojis)
                        clean_channel_name = re.sub(r'[^\w\s-]', '', resolved_channel)
                        
                        # Formatting
                        plt.title(f'Daily Message Activity - {clean_channel_name}', fontsize=16, fontweight='bold', pad=20)
                        plt.xlabel('Date', fontsize=12)
                        plt.ylabel('Number of Messages', fontsize=12)
                        
                        # Format x-axis
                        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))
                        plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=max(1, len(dates)//10)))
                        plt.xticks(rotation=45)
                        
                        # Add grid for better readability
                        plt.grid(True, alpha=0.3, axis='y')
                        
                        # Add some statistics to the plot
                        avg_messages = sum(message_counts) / len(message_counts)
                        max_messages = max(message_counts)
                        
                        plt.axhline(y=avg_messages, color='red', linestyle='--', alpha=0.7, 
                                  label=f'Average: {avg_messages:.1f} msg/day')
                        
                        plt.legend()
                        plt.tight_layout()
                        
                        # Save the chart with sanitized filename
                        safe_channel_name = re.sub(r'[^\w\s-]', '', resolved_channel).replace(' ', '_').strip('_')
                        if not safe_channel_name:
                            safe_channel_name = "unknown_channel"
                        
                        chart_filename = f"channel_activity_{safe_channel_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                        chart_path = os.path.join('temp', chart_filename)
                        
                        # Ensure temp directory exists
                        os.makedirs('temp', exist_ok=True)
                        
                        plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
                        plt.close()
                        
            except Exception as e:
                print(f"Error generating activity chart: {e}")
            
            # Format results
            result = f"**Channel Analysis: #{resolved_channel}**\n\n"
            
            # Basic Statistics
            result += f"**Basic Statistics:**\n"
            result += f"â€¢ Total Messages: {total_messages}\n"
            result += f"â€¢ Unique Users: {unique_users}\n"
            result += f"â€¢ Average Message Length: {avg_length:.1f} characters\n"
            
            if first_msg and last_msg:
                try:
                    from datetime import datetime
                    first_dt = datetime.fromisoformat(first_msg.replace('Z', '+00:00'))
                    last_dt = datetime.fromisoformat(last_msg.replace('Z', '+00:00'))
                    result += f"â€¢ First Message: {first_dt.strftime('%Y-%m-%d %H:%M')}\n"
                    result += f"â€¢ Last Message: {last_dt.strftime('%Y-%m-%d %H:%M')}\n"
                except:
                    result += f"â€¢ First Message: {self.format_timestamp(first_msg)}\n"
                    result += f"â€¢ Last Message: {self.format_timestamp(last_msg)}\n"
            
            result += "\n"
            
            # Engagement Metrics
            result += f"**ðŸ“ˆ Engagement Metrics:**\n"
            result += f"â€¢ Average Replies per Original Post: {replies_per_post:.2f}\n"
            result += f"â€¢ Posts with Reactions: {reaction_rate:.1f}% ({posts_with_reactions}/{total_messages})\n"
            result += f"â€¢ Total Replies: {total_replies} | Original Posts: {original_posts}\n\n"
            
            # Top Contributors
            if contributors:
                result += f"**Top Contributors:**\n"
                for name, count, avg_chars in contributors:
                    result += f"â€¢ {name}: {count} messages (avg {avg_chars:.0f} chars)\n"
                result += "\n"
            
            # Peak Activity Hours
            if peak_hours:
                result += f"**Peak Activity Hours:**\n"
                for hour, count in peak_hours:
                    result += f"â€¢ {hour}:00-{hour}:59: {count} messages\n"
                result += "\n"
            
            # Activity by Day
            if day_activity:
                result += f"**Activity by Day:**\n"
                for day, count in day_activity:
                    result += f"â€¢ {day}: {count} messages\n"
                result += "\n"
            
            # Recent Activity
            if recent_activity:
                result += f"**Recent Activity (Last 7 Days):**\n"
                for date, count in recent_activity:
                    result += f"â€¢ {date}: {count} messages\n"
                result += "\n"
            
            # Channel Health Metrics
            result += f"**ðŸ“ˆ Channel Health Metrics:**\n"
            
            if total_channel_members > 0:
                # Enhanced metrics with full membership data
                result += f"â€¢ Total Channel Members: {total_channel_members}\n"
                result += f"â€¢ Members Who Ever Posted: {unique_users} ({participation_rate:.1f}%)\n"
                result += f"â€¢ Weekly Active Members: {weekly_active} ({(weekly_active/total_channel_members*100):.1f}% of total)\n"
                result += f"â€¢ Recently Inactive Members: {inactive_users} ({inactive_percentage:.1f}% of posters)\n"
                result += f"â€¢ Lurkers (Never Posted): {lurkers} ({(lurkers/total_channel_members*100):.1f}%)\n"
                result += f"â€¢ Participation Rate: {participation_rate:.1f}% (members who have posted)\n"
                result += f"â€¢ Activity Ratio: {weekly_active} active / {inactive_users} inactive / {lurkers} lurkers\n\n"
            else:
                # Fallback to message-based metrics
                result += f"â€¢ Total Members Ever Active: {unique_users}\n"
                result += f"â€¢ Weekly Active Members: {weekly_active} ({channel_amp:.1f}%)\n"
                result += f"â€¢ Recently Inactive Members: {inactive_users} ({inactive_percentage:.1f}%)\n"
                result += f"â€¢ Activity Ratio: {weekly_active} active / {inactive_users} inactive\n"
                result += f"â€¢ Note: Full membership data not available - showing message-based metrics only\n\n"
            
            # Top Topics Discussed
            if top_topics:
                result += f"**ðŸ§  Top Topics Discussed:**\n"
                for i, topic in enumerate(top_topics, 1):
                    result += f"{i}. {topic}\n"
            
            # Return both text and chart path if chart was generated
            if chart_path and os.path.exists(chart_path):
                return (result, chart_path)
            else:
                return result
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"Error getting channel insights: {str(e)}"